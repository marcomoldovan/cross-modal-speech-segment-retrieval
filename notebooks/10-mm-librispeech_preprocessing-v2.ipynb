{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, BertTokenizerFast, BertModel\n",
    "from transformers.models.hubert.modeling_hubert import HubertPreTrainedModel, HubertFeatureEncoder\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubertConvFeatureExtractorWrapper(HubertPreTrainedModel):\n",
    "    # named HubertFeatureEncoder on huggingface\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "        ):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.feature_extractor = HubertFeatureEncoder(config)\n",
    "        \n",
    "        self.post_init()\n",
    "    \n",
    "    \n",
    "    def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n",
    "        return self.feature_extractor(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        max_length: int = 16,\n",
    "        dataset_name: str = 'librispeech_asr',\n",
    "        save_dir: str = 'E:/Datasets/',\n",
    "        text_model_name: str = 'google/bert_uncased_L-2_H-768_A-12',\n",
    "    ):\n",
    "        assert torch.cuda.is_available(), \"CUDA is not available, should run on GPU\"\n",
    "        \n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('ntu-spml/distilhubert')\n",
    "        self.feature_encoder = HubertConvFeatureExtractorWrapper.from_pretrained('ntu-spml/distilhubert')\n",
    "        self.feature_encoder.eval()\n",
    "        \n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(text_model_name)\n",
    "        self.text_model = BertModel.from_pretrained(text_model_name)\n",
    "        self.text_model.eval()\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.cache_dir = save_dir\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        self.max_length = max_length*16000\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "                \n",
    "        self.dataset = None\n",
    "        \n",
    "        \n",
    "    def load_dataset(self, dataset_split: str = 'train.360'):\n",
    "        self.dataset = load_dataset(self.dataset_name, 'clean', split=dataset_split, cache_dir=self.cache_dir)\n",
    "        \n",
    "        \n",
    "    def _speech_file_to_array(self, data):\n",
    "        data['speech'] = data['audio']['array']\n",
    "        data['sampling_rate'] = data['audio']['sampling_rate']\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def speech_file_to_array(self, dataset=None, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._speech_file_to_array, \n",
    "            remove_columns=['file', 'audio', 'speaker_id', 'chapter_id', 'id']\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}file_to_speech_array/')\n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    def filter_long_audio(self, dataset=None, max_audio_length: int = 16, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.dataset = self.dataset.filter(\n",
    "            lambda x: len(x['speech'])//x['sampling_rate'] < max_audio_length, \n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}filtered/')\n",
    "        return self.dataset\n",
    "        \n",
    "        \n",
    "    def _extract_features_and_tokenize(self, data):\n",
    "        # check that all files have the correct sampling rate\n",
    "        assert (\n",
    "            len(set(data['sampling_rate'])) == 1\n",
    "        ), f\"Make sure all inputs have the same sampling rate of {self.feature_extractor.sampling_rate}.\"\n",
    "        \n",
    "        # extract and pad input values\n",
    "        input_values = self.feature_extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "        data['input_values'] = input_values.input_values\n",
    "        padded_input_values = self.feature_extractor(data['speech'], padding=True, return_tensors='pt', sampling_rate=data['sampling_rate'][0])\n",
    "        \n",
    "        # compute the latent features from the conv module\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            input_values = padded_input_values['input_values'].to(self.device)\n",
    "            latent_features = self.feature_encoder(input_values).transpose(1, 2)\n",
    "            latent_features = latent_features.cpu().numpy()\n",
    "            data['latent_features'] = latent_features\n",
    "            \n",
    "        # tokenize text\n",
    "        tokenized_batch = self.tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "        data['input_ids'] = tokenized_batch['input_ids']\n",
    "        data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "        data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "        \n",
    "        return data\n",
    "\n",
    "            \n",
    "        \n",
    "    def extract_features_and_tokenize(self, dataset=None, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.feature_encoder.cuda()\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._extract_features_and_tokenize, \n",
    "            batch_size=16, \n",
    "            num_proc=1, \n",
    "            batched=True, \n",
    "            remove_columns=['text', 'sampling_rate'],\n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}features_and_tokens/')\n",
    "        self.feature_encoder.cpu()\n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    def _encode_text(self, data):\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            input_ids = torch.tensor(data['input_ids'], dtype=torch.int, device=self.device)\n",
    "            attention_mask = torch.tensor(data['attention_mask_text'], dtype=torch.int, device=self.device)\n",
    "            token_type_ids = torch.tensor(data['token_type_ids_text'], dtype=torch.int, device=self.device)\n",
    "            embeddings = self.text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask, \n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            embeddings = embeddings.pooler_output.cpu().numpy()\n",
    "            data['sentence_embedding'] = embeddings\n",
    "            \n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def encode_text(self, dataset=None, save_to_hd: bool = True, shard=0):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.text_model.cuda()\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._encode_text, \n",
    "            batch_size=16, \n",
    "            num_proc=1, \n",
    "            batched=True, \n",
    "            remove_columns=['input_ids', 'attention_mask_text', 'token_type_ids_text'],\n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}encoded/{shard}/')\n",
    "        self.text_model.cpu()\n",
    "    \n",
    "    \n",
    "    def save_dataset(\n",
    "        self, \n",
    "        save_in: str,\n",
    "        save_path: str,\n",
    "    ):\n",
    "        self.dataset.save_to_disk(f'{save_in}/librispeech_asr_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (E:/Datasets/librispeech/original/librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('librispeech_asr', 'clean', split='train.360', cache_dir='E:/Datasets/librispeech/original/') # <-- this is the correct way to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = LibriPreprocessor(dataset=None, save_dir='E:/Datasets/librispeech/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104014/104014 [45:03<00:00, 38.48ex/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset_file_to_speech_array = preprocessor.speech_file_to_array(dataset=dataset, save_to_hd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_speech_array = load_from_disk('E:/Datasets/librispeech/file_to_speech_array/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_bits = file_to_speech_array.shard(128, 64)\n",
    "len(little_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.54s/ba]\n"
     ]
    }
   ],
   "source": [
    "little_bits_filtered = little_bits.filter(lambda x: len(x['speech'])//x['sampling_rate'] < 16, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(little_bits_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 64\n",
    "\n",
    "for i in range(num_shards-0):\n",
    "    libri_shard = file_to_speech_array.shard(num_shards, i)\n",
    "    print(f\"Shard {i} has {len(libri_shard)} entries.\")\n",
    "    libri_filtered = preprocessor.filter_long_audio(dataset=libri_shard, max_audio_length=16, save_to_hd=False)\n",
    "    print(f\"Shard {i} has been filtered to {len(libri_filtered)} entries.\")\n",
    "    libri_audio_features = preprocessor.extract_features_and_tokenize(dataset=libri_filtered, save_to_hd=False)\n",
    "    print(f\"Shard {i} has had features extracted from audio file.\")\n",
    "    libri_encoded = preprocessor.encode_text(dataset=libri_audio_features, save_to_hd=True, shard=i)\n",
    "    print(f'Shard {i} has had text encoded and was saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('ntu-spml/distilhubert')\n",
    "feature_encoder = HubertConvFeatureExtractorWrapper.from_pretrained('ntu-spml/distilhubert')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('google/bert_uncased_L-2_H-768_A-12')\n",
    "text_model = BertModel.from_pretrained('google/bert_uncased_L-2_H-768_A-12')\n",
    "\n",
    "max_length = 16*16000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _extract_fts(data):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(data['sampling_rate'])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {feature_extractor.sampling_rate}.\"\n",
    "    # extract and pad input values\n",
    "    input_values = feature_extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "    data['input_values'] = input_values.input_values\n",
    "    padded_input_values = feature_extractor(data['speech'], padding=True, return_tensors='pt', sampling_rate=data['sampling_rate'][0])\n",
    "    \n",
    "    # compute the latent features from the conv module\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        input_values = padded_input_values['input_values'].to(device)\n",
    "        latent_features = feature_encoder(input_values).transpose(1, 2)\n",
    "        latent_features = latent_features.cpu().numpy()\n",
    "        data['latent_features'] = latent_features\n",
    "        \n",
    "    # tokenize text\n",
    "    tokenized_batch = tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "    data['input_ids'] = tokenized_batch['input_ids']\n",
    "    data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "    data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "    \n",
    "def extract_fts(dataset):\n",
    "    feature_encoder.cuda()\n",
    "    dataset = dataset.map(\n",
    "        _extract_fts, \n",
    "        batch_size=16, \n",
    "        num_proc=1, \n",
    "        batched=True, \n",
    "        remove_columns=['text', 'sampling_rate'],\n",
    "        keep_in_memory=True\n",
    "    )\n",
    "    feature_encoder.cpu()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _encode_text(data):\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(data['input_ids'], dtype=torch.int, device=device)\n",
    "        attention_mask = torch.tensor(data['attention_mask_text'], dtype=torch.int, device=device)\n",
    "        token_type_ids = torch.tensor(data['token_type_ids_text'], dtype=torch.int, device=device)\n",
    "        embeddings = text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        embeddings = embeddings.pooler_output.cpu().numpy()\n",
    "        data['sentence_embedding'] = embeddings\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "def encode_text(dataset=None):\n",
    "    text_model.cuda()\n",
    "    dataset = dataset.map(\n",
    "        _encode_text, \n",
    "        batch_size=16, \n",
    "        num_proc=1, \n",
    "        batched=True, \n",
    "        remove_columns=['input_ids', 'attention_mask_text', 'token_type_ids_text'],\n",
    "        keep_in_memory=True\n",
    "    )\n",
    "    text_model.cpu()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_bits_fts = extract_fts(dataset=little_bits_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [04:36<00:00,  5.76s/ba]\n"
     ]
    }
   ],
   "source": [
    "little_bits_encoded = encode_text(dataset=little_bits_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speech', 'input_values', 'latent_features', 'sentence_embedding'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_bits_encoded[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('speech-segment-retrieval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa0b5aecd04a9a200839e508d6e55aa58d0ad138d096dd077c8ef9e50abc1435"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
