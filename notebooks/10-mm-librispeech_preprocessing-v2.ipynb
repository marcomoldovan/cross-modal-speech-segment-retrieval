{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, BertTokenizerFast, BertModel\n",
    "from transformers.models.hubert.modeling_hubert import HubertPreTrainedModel, HubertFeatureEncoder\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubertConvFeatureExtractorWrapper(HubertPreTrainedModel):\n",
    "    # named HubertFeatureEncoder on huggingface\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "        ):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.feature_extractor = HubertFeatureEncoder(config)\n",
    "        \n",
    "        self.post_init()\n",
    "    \n",
    "    \n",
    "    def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n",
    "        return self.feature_extractor(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        max_length: int = 16,\n",
    "        dataset_name: str = 'librispeech_asr',\n",
    "        save_dir: str = 'E:/Datasets/',\n",
    "        text_model_name: str = 'google/bert_uncased_L-2_H-768_A-12',\n",
    "    ):\n",
    "        assert torch.cuda.is_available(), \"CUDA is not available, should run on GPU\"\n",
    "        \n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('ntu-spml/distilhubert')\n",
    "        self.feature_encoder = HubertConvFeatureExtractorWrapper.from_pretrained('ntu-spml/distilhubert')\n",
    "        self.feature_encoder.eval()\n",
    "        \n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(text_model_name)\n",
    "        self.text_model = BertModel.from_pretrained(text_model_name)\n",
    "        self.text_model.eval()\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.cache_dir = save_dir\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        self.max_length = max_length*16000\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "                \n",
    "        self.dataset = None\n",
    "        \n",
    "        \n",
    "    def load_dataset(self, dataset_split: str = 'train.360'):\n",
    "        self.dataset = load_dataset(self.dataset_name, 'clean', split=dataset_split, cache_dir=self.cache_dir)\n",
    "        \n",
    "        \n",
    "    def _speech_file_to_array(self, data):\n",
    "        data['speech'] = data['audio']['array']\n",
    "        data['sampling_rate'] = data['audio']['sampling_rate']\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def speech_file_to_array(self, dataset=None, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._speech_file_to_array, \n",
    "            remove_columns=['file', 'audio', 'speaker_id', 'chapter_id', 'id']\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}file_to_speech_array/')\n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    def filter_long_audio(self, dataset=None, max_audio_length: int = 16, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.dataset = self.dataset.filter(\n",
    "            lambda x: len(x['speech'])//x['sampling_rate'] < max_audio_length, \n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}filtered/')\n",
    "        return self.dataset\n",
    "        \n",
    "        \n",
    "    def _extract_features_and_tokenize(self, data):\n",
    "        # check that all files have the correct sampling rate\n",
    "        assert (\n",
    "            len(set(data['sampling_rate'])) == 1\n",
    "        ), f\"Make sure all inputs have the same sampling rate of {self.feature_extractor.sampling_rate}.\"\n",
    "        \n",
    "        # extract and pad input values\n",
    "        input_values = self.feature_extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "        data['input_values'] = input_values.input_values\n",
    "        padded_input_values = self.feature_extractor(data['speech'], padding=True, return_tensors='pt', sampling_rate=data['sampling_rate'][0])\n",
    "        \n",
    "        # compute the latent features from the conv module\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            input_values = padded_input_values['input_values'].to(self.device)\n",
    "            latent_features = self.feature_encoder(input_values).transpose(1, 2)\n",
    "            latent_features = latent_features.cpu().numpy()\n",
    "            data['latent_features'] = latent_features\n",
    "            \n",
    "        # tokenize text\n",
    "        tokenized_batch = self.tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "        data['input_ids'] = tokenized_batch['input_ids']\n",
    "        data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "        data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "        \n",
    "        return data\n",
    "\n",
    "            \n",
    "        \n",
    "    def extract_features_and_tokenize(self, dataset=None, save_to_hd: bool = True):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.feature_encoder.cuda()\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._extract_features_and_tokenize, \n",
    "            batch_size=16, \n",
    "            num_proc=1, \n",
    "            batched=True, \n",
    "            remove_columns=['text', 'sampling_rate'],\n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}features_and_tokens/')\n",
    "        self.feature_encoder.cpu()\n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    def _encode_text(self, data):\n",
    "        import torch\n",
    "        with torch.no_grad():\n",
    "            input_ids = torch.tensor(data['input_ids'], dtype=torch.int, device=self.device)\n",
    "            attention_mask = torch.tensor(data['attention_mask_text'], dtype=torch.int, device=self.device)\n",
    "            token_type_ids = torch.tensor(data['token_type_ids_text'], dtype=torch.int, device=self.device)\n",
    "            embeddings = self.text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask, \n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            embeddings = embeddings.pooler_output.cpu().numpy()\n",
    "            data['sentence_embedding'] = embeddings\n",
    "            \n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def encode_text(self, dataset=None, save_to_hd: bool = True, shard=0):\n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "        self.text_model.cuda()\n",
    "        self.dataset = self.dataset.map(\n",
    "            self._encode_text, \n",
    "            batch_size=16, \n",
    "            num_proc=1, \n",
    "            batched=True, \n",
    "            remove_columns=['input_ids', 'attention_mask_text', 'token_type_ids_text'],\n",
    "            keep_in_memory=True\n",
    "        )\n",
    "        if save_to_hd:\n",
    "            self.dataset.save_to_disk(f'{self.save_dir}encoded/{shard}/')\n",
    "        self.text_model.cpu()\n",
    "    \n",
    "    \n",
    "    def save_dataset(\n",
    "        self, \n",
    "        save_in: str,\n",
    "        save_path: str,\n",
    "    ):\n",
    "        self.dataset.save_to_disk(f'{save_in}/librispeech_asr_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (E:/Datasets/librispeech/original/librispeech_asr\\clean\\2.1.0\\14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('librispeech_asr', 'clean', split='train.360', cache_dir='E:/Datasets/librispeech/original/') # <-- this is the correct way to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ntu-spml/distilhubert were not used when initializing HubertConvFeatureExtractorWrapper: ['encoder.layers.0.attention.out_proj.bias', 'feature_projection.projection.weight', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.1.attention.out_proj.bias', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layer_norm.bias', 'encoder.layers.0.attention.k_proj.weight', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.1.attention.k_proj.weight', 'encoder.pos_conv_embed.conv.weight_v', 'encoder.layers.0.attention.v_proj.weight', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.1.attention.out_proj.weight', 'encoder.pos_conv_embed.conv.bias', 'encoder.pos_conv_embed.conv.weight_g', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'feature_projection.projection.bias', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.feed_forward.output_dense.bias', 'masked_spec_embed', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.feed_forward.intermediate_dense.weight']\n",
      "- This IS expected if you are initializing HubertConvFeatureExtractorWrapper from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertConvFeatureExtractorWrapper from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-768_A-12 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "preprocessor = LibriPreprocessor(dataset=None, save_dir='E:/Datasets/librispeech/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104014/104014 [45:03<00:00, 38.48ex/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset_file_to_speech_array = preprocessor.speech_file_to_array(dataset=dataset, save_to_hd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_speech_array = load_from_disk('E:/Datasets/librispeech/file_to_speech_array/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_bits = file_to_speech_array.shard(128, 64)\n",
    "len(little_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:28<00:00, 88.54s/ba]\n"
     ]
    }
   ],
   "source": [
    "little_bits_filtered = little_bits.filter(lambda x: len(x['speech'])//x['sampling_rate'] < 16, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(little_bits_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:01<00:00, 60.61s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0 has been filtered to 1532 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?ba/s]c:\\Users\\marco\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "100%|██████████| 96/96 [03:41<00:00,  2.30s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:34<00:00,  5.98s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0 has had text encoded and was saved to disk.\n",
      "Shard 1 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:08<00:00, 64.32s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1 has been filtered to 1529 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:48<00:00,  2.38s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:28<00:00,  5.92s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1 has had text encoded and was saved to disk.\n",
      "Shard 2 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:41<00:00, 80.93s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2 has been filtered to 1524 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:56<00:00,  2.46s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:27<00:00,  5.91s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2 has had text encoded and was saved to disk.\n",
      "Shard 3 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:46<00:00, 83.20s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3 has been filtered to 1521 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:49<00:00,  2.39s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:30<00:00,  5.94s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3 has had text encoded and was saved to disk.\n",
      "Shard 4 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:42<00:00, 81.16s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 4 has been filtered to 1517 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [03:51<00:00,  2.43s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 4 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [09:19<00:00,  5.89s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 4 has had text encoded and was saved to disk.\n",
      "Shard 5 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:41<00:00, 80.80s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 5 has been filtered to 1530 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:50<00:00,  2.40s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 5 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:31<00:00,  5.95s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 5 has had text encoded and was saved to disk.\n",
      "Shard 6 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:43<00:00, 81.74s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 6 has been filtered to 1525 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:57<00:00,  2.47s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 6 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:31<00:00,  5.96s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 6 has had text encoded and was saved to disk.\n",
      "Shard 7 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:42<00:00, 81.22s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 7 has been filtered to 1532 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:54<00:00,  2.44s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 7 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:28<00:00,  5.92s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 7 has had text encoded and was saved to disk.\n",
      "Shard 8 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:40<00:00, 80.32s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 8 has been filtered to 1530 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:51<00:00,  2.41s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 8 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:37<00:00,  6.02s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 8 has had text encoded and was saved to disk.\n",
      "Shard 9 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:47<00:00, 83.74s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 9 has been filtered to 1532 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [04:17<00:00,  2.69s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 9 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:16<00:00,  5.80s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 9 has had text encoded and was saved to disk.\n",
      "Shard 10 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:40<00:00, 80.15s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 10 has been filtered to 1522 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:53<00:00,  2.43s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 10 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:32<00:00,  5.96s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 10 has had text encoded and was saved to disk.\n",
      "Shard 11 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:42<00:00, 81.29s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 11 has been filtered to 1507 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [03:47<00:00,  2.40s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 11 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [09:28<00:00,  5.99s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 11 has had text encoded and was saved to disk.\n",
      "Shard 12 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:45<00:00, 82.73s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 12 has been filtered to 1528 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:55<00:00,  2.45s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 12 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:06<00:00,  5.70s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 12 has had text encoded and was saved to disk.\n",
      "Shard 13 has 1626 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:46<00:00, 83.30s/ba] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 13 has been filtered to 1522 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [04:02<00:00,  2.52s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 13 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:27<00:00,  5.91s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 13 has had text encoded and was saved to disk.\n",
      "Shard 14 has 1625 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:42<00:00, 81.22s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 14 has been filtered to 1536 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:56<00:00,  2.46s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 14 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [09:27<00:00,  5.91s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 14 has had text encoded and was saved to disk.\n",
      "Shard 15 has 1625 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:37<00:00, 78.99s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 15 has been filtered to 1537 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [03:54<00:00,  2.42s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 15 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [08:56<00:00,  5.53s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 15 has had text encoded and was saved to disk.\n",
      "Shard 16 has 1625 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:37<00:00, 78.94s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 16 has been filtered to 1522 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [03:43<00:00,  2.32s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 16 has had features extracted from audio file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [08:54<00:00,  5.57s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 16 has had text encoded and was saved to disk.\n",
      "Shard 17 has 1625 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?ba/s]"
     ]
    }
   ],
   "source": [
    "num_shards = 64\n",
    "\n",
    "for i in range(num_shards-0):\n",
    "    libri_shard = file_to_speech_array.shard(num_shards, i)\n",
    "    print(f\"Shard {i} has {len(libri_shard)} entries.\")\n",
    "    libri_filtered = preprocessor.filter_long_audio(dataset=libri_shard, max_audio_length=16, save_to_hd=False)\n",
    "    print(f\"Shard {i} has been filtered to {len(libri_filtered)} entries.\")\n",
    "    libri_audio_features = preprocessor.extract_features_and_tokenize(dataset=libri_filtered, save_to_hd=False)\n",
    "    print(f\"Shard {i} has had features extracted from audio file.\")\n",
    "    libri_encoded = preprocessor.encode_text(dataset=libri_audio_features, save_to_hd=True, shard=i)\n",
    "    print(f'Shard {i} has had text encoded and was saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ntu-spml/distilhubert were not used when initializing HubertConvFeatureExtractorWrapper: ['encoder.layers.0.attention.out_proj.bias', 'feature_projection.projection.weight', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.1.attention.out_proj.bias', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layer_norm.bias', 'encoder.layers.0.attention.k_proj.weight', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.1.attention.k_proj.weight', 'encoder.pos_conv_embed.conv.weight_v', 'encoder.layers.0.attention.v_proj.weight', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.1.attention.out_proj.weight', 'encoder.pos_conv_embed.conv.bias', 'encoder.pos_conv_embed.conv.weight_g', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'feature_projection.projection.bias', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.feed_forward.output_dense.bias', 'masked_spec_embed', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.feed_forward.intermediate_dense.weight']\n",
      "- This IS expected if you are initializing HubertConvFeatureExtractorWrapper from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertConvFeatureExtractorWrapper from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-768_A-12 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('ntu-spml/distilhubert')\n",
    "feature_encoder = HubertConvFeatureExtractorWrapper.from_pretrained('ntu-spml/distilhubert')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('google/bert_uncased_L-2_H-768_A-12')\n",
    "text_model = BertModel.from_pretrained('google/bert_uncased_L-2_H-768_A-12')\n",
    "\n",
    "max_length = 16*16000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _extract_fts(data):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(data['sampling_rate'])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {feature_extractor.sampling_rate}.\"\n",
    "    # extract and pad input values\n",
    "    input_values = feature_extractor(data['speech'], sampling_rate=data['sampling_rate'][0])\n",
    "    data['input_values'] = input_values.input_values\n",
    "    padded_input_values = feature_extractor(data['speech'], padding=True, return_tensors='pt', sampling_rate=data['sampling_rate'][0])\n",
    "    \n",
    "    # compute the latent features from the conv module\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        input_values = padded_input_values['input_values'].to(device)\n",
    "        latent_features = feature_encoder(input_values).transpose(1, 2)\n",
    "        latent_features = latent_features.cpu().numpy()\n",
    "        data['latent_features'] = latent_features\n",
    "        \n",
    "    # tokenize text\n",
    "    tokenized_batch = tokenizer(data['text'], padding='longest', max_length=128, pad_to_max_length=False)\n",
    "    data['input_ids'] = tokenized_batch['input_ids']\n",
    "    data['attention_mask_text'] = tokenized_batch['attention_mask']\n",
    "    data['token_type_ids_text'] = tokenized_batch['token_type_ids']\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "    \n",
    "def extract_fts(dataset):\n",
    "    feature_encoder.cuda()\n",
    "    dataset = dataset.map(\n",
    "        _extract_fts, \n",
    "        batch_size=16, \n",
    "        num_proc=1, \n",
    "        batched=True, \n",
    "        remove_columns=['text', 'sampling_rate'],\n",
    "        keep_in_memory=True\n",
    "    )\n",
    "    feature_encoder.cpu()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _encode_text(data):\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(data['input_ids'], dtype=torch.int, device=device)\n",
    "        attention_mask = torch.tensor(data['attention_mask_text'], dtype=torch.int, device=device)\n",
    "        token_type_ids = torch.tensor(data['token_type_ids_text'], dtype=torch.int, device=device)\n",
    "        embeddings = text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        embeddings = embeddings.pooler_output.cpu().numpy()\n",
    "        data['sentence_embedding'] = embeddings\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "def encode_text(dataset=None):\n",
    "    text_model.cuda()\n",
    "    dataset = dataset.map(\n",
    "        _encode_text, \n",
    "        batch_size=16, \n",
    "        num_proc=1, \n",
    "        batched=True, \n",
    "        remove_columns=['input_ids', 'attention_mask_text', 'token_type_ids_text'],\n",
    "        keep_in_memory=True\n",
    "    )\n",
    "    text_model.cpu()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?ba/s]c:\\Users\\marco\\.venv\\speech-segment-retrieval\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "100%|██████████| 48/48 [01:43<00:00,  2.15s/ba]\n"
     ]
    }
   ],
   "source": [
    "little_bits_fts = extract_fts(dataset=little_bits_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [04:36<00:00,  5.76s/ba]\n"
     ]
    }
   ],
   "source": [
    "little_bits_encoded = encode_text(dataset=little_bits_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speech', 'input_values', 'latent_features', 'sentence_embedding'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_bits_encoded[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('speech-segment-retrieval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa0b5aecd04a9a200839e508d6e55aa58d0ad138d096dd077c8ef9e50abc1435"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
